[
  {
    "source": "Homework_ The Lecture-Saver 3000.pdf",
    "page": 1,
    "text": "üéì Homework: The Lecture-Saver 3000 Building a RAG-Based Academic Assistant The Scenario As final-year AI students, you know that LLMs are powerful reasoning engines, but they lack specific knowledge about your curriculum. They know what \"Machine Learning\" is in general, but they don't know exactly what your professor said about \"Gradient Descent\" in Week 3. Your Task: Select a specific subject from this semester (e.g., Natural Language Processing, Computer Vision, or Machine Learning). Gather the PDF lectures or notes for that subject. You will build a Retrieval-Augmented Generation (RAG) pipeline that digests these specific materials and allows you to chat with them. The goal is to create a tool that helps you study by providing answers grounded strictly in your course material. The Core Requirements Requirement 1: The Ingestion Pipeline (Data Preparation) Context: An LLM is only as good as the data you feed it. Simply pasting text isn't enough; you need to structure the data so it can be retrieved accurately later. You need to handle Metadata (knowing where the text came from). Your Task: 1. Load: Write a script to read text from your chosen PDF/Text files. 2. Chunk: Split the text into manageable pieces (e.g., 500-1000 characters). 3. Embed & Index: Convert chunks to vectors and store them in a Vector Database (like ChromaDB, FAISS, or Qdrant). 4. Critical: You must store the metadata (Filename and Page Number) alongside the vector. üí° Example: Input: A PDF file named Lecture_05_Transformers.pdf.",
    "chunk_id": 0
  },
  {
    "source": "Homework_ The Lecture-Saver 3000.pdf",
    "page": 2,
    "text": "Process: You extract the text from Page 12. Storage: In your database, the record should look something like this: code JSON downloadcontent_copy expand_less { \"id\": \"chunk_45\", \"vector\": [0.12, -0.98, 0.45, ...], \"text_content\": \"The attention mechanism allows the model to focus on...\", \"metadata\": {\"source\": \"Lecture_05_Transformers.pdf\", \"page\": 12} } Requirement 2: The Retrieval & Chat Pipeline Context: Now you need to build the \"brain\" of the application. This involves the \"R\" (Retrieval) and \"G\" (Generation) of RAG. You must use a free API provider (Groq, Gemini, or OpenRouter) to handle the generation. Your Task: 1. Retrieve: When a user asks a question, embed the question and find the top 3-5 most similar chunks from your database. 2. System Prompting: Create a prompt that forces the LLM to act as a \"University Tutor.\" It should strictly use the provided context to answer. 3. Generate: Pass the Context + Question to the LLM and print the response. üí° Example: User Query: \"What is the difference between Encoder and Decoder?\" System Action: Retrieves 3 chunks from Lecture_05 and Lecture_06. Constructed Prompt: \"You are a helpful tutor. Use the following context to answer the student's question. Context: [Insert Retrieved Text]. Question: What is the difference between Encoder and Decoder?\" Requirement 3: Evidence-Based Answers (Citations) Context: In academia, an answer without a source is just an opinion. A major problem with LLMs is \"hallucination.\" To solve this, your bot must provide citations for every answer it gives.",
    "chunk_id": 1
  },
  {
    "source": "Homework_ The Lecture-Saver 3000.pdf",
    "page": 3,
    "text": "Your Task: Modify your output logic so that after the LLM generates the answer, your code appends the source information found in the metadata of the retrieved chunks. üí° Example Output: Student: \"How do we calculate the F1-Score?\" Lecture-Saver 3000: \"The F1-Score is the harmonic mean of Precision and Recall. It is calculated as 2 * (Precision * Recall) / (Precision + Recall). It is useful when classes are imbalanced.\" üîç Sources: 1. Lecture_02_Metrics.pdf (Page 15) 2. Lab_03_Classification.pdf (Page 4) Bonus: The \"Product\" Touch (Optional +10 Marks) Take your code out of the notebook and make it a real app. Use Streamlit (Python library) to create a simple User Interface where the user can: 1. Upload a PDF file via the browser. 2. Chat with it in a message window. Note: Streamlit is very easy to use and requires no HTML/CSS knowledge. Recommended Resources & Stack Since you are students, utilize Free Tier services: 1. LLM Provider (Choose one): ‚óã Groq Cloud: Offers Llama-3-70b with incredibly fast speeds (Free). ‚óã Google AI Studio: Offers Gemini-1.5-Flash (Free). ‚óã Mistral API: Offers Mistral-Small (Free tier). 2. Embeddings: Use sentence-transformers (runs locally on your CPU) or Google's embedding model. 3. Vector DB: chromadb (easiest to set up in a notebook). 4. PDF Parsing: pypdf or langchain_community.document_loaders.",
    "chunk_id": 2
  },
  {
    "source": "Homework_ The Lecture-Saver 3000.pdf",
    "page": 4,
    "text": "üìù Deliverables Submit a generic Github repository link or a ZIP file containing: 1. rag_pipeline.py or notebook.ipynb: The code containing ingestion and chat logic. 2. requirements.txt: The libraries used. 3. A short Report (PDF): ‚óã Which model did you use? ‚óã What chunk_size did you choose and why? ‚óã Screenshot of the bot answering a question with the correct citation. Good luck, and may the vectors be with you! ü§ñ",
    "chunk_id": 3
  }
]